{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 4: Multi-factor Model\n",
    "## Instructions\n",
    "Each problem consists of a function to implement and instructions on how to implement the function.  The parts of the function that need to be implemented are marked with a `# TODO` comment. After implementing the function, run the cell to test it against the unit tests we've provided. For each problem, we provide one or more unit tests from our `project_tests` package. These unit tests won't tell you if your answer is correct, but will warn you of any major errors. Your code will be checked for the correct solution when you submit it to Udacity.\n",
    "\n",
    "## Packages\n",
    "When you implement the functions, you'll only need to you use the packages you've used in the classroom, like [Pandas](https://pandas.pydata.org/) and [Numpy](http://www.numpy.org/). These packages will be imported for you. We recommend you don't add any import statements, otherwise the grader might not be able to run your code.\n",
    "\n",
    "The other packages that we're importing are `project_helper` and `project_tests`. These are custom packages built to help you solve the problems.  The `project_helper` module contains utility functions and graph functions. The `project_tests` contains the unit tests for all the problems.\n",
    "\n",
    "### Install Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting alphalens==0.3.2\n",
      "  Downloading alphalens-0.3.2.tar.gz (18.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 18.9 MB 263 kB/s eta 0:00:01     |██████████████████████▎         | 13.1 MB 3.3 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting colour==0.1.5\n",
      "  Downloading colour-0.1.5-py2.py3-none-any.whl (23 kB)\n",
      "Collecting cvxpy==1.0.3\n",
      "  Downloading cvxpy-1.0.3.tar.gz (880 kB)\n",
      "\u001b[K     |████████████████████████████████| 880 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cycler==0.10.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (0.10.0)\n",
      "Collecting numpy==1.18.1\n",
      "  Downloading numpy-1.18.1-cp36-cp36m-macosx_10_9_x86_64.whl (15.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.2 MB 540 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pandas==0.18.1\n",
      "  Downloading pandas-0.18.1.tar.gz (7.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.3 MB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plotly==2.2.3\n",
      "  Downloading plotly-2.2.3.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing==2.2.0\n",
      "  Downloading pyparsing-2.2.0-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting python-dateutil==2.6.1\n",
      "  Downloading python_dateutil-2.6.1-py2.py3-none-any.whl (194 kB)\n",
      "\u001b[K     |████████████████████████████████| 194 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz==2017.3\n",
      "  Downloading pytz-2017.3-py2.py3-none-any.whl (511 kB)\n",
      "\u001b[K     |████████████████████████████████| 511 kB 1.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests==2.18.4\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scipy==1.0.0\n",
      "  Downloading scipy-1.0.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (16.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 16.7 MB 299 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting scikit-learn==0.19.1\n",
      "  Downloading scikit_learn-0.19.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (7.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.6 MB 3.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting six==1.11.0\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting tables==3.3.0\n",
      "  Downloading tables-3.3.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 1.1 MB/s eta 0:00:01     |███████████████████████████▍    | 4.1 MB 227 kB/s eta 0:00:04\n",
      "\u001b[?25hCollecting tqdm==4.19.5\n",
      "  Downloading tqdm-4.19.5-py2.py3-none-any.whl (51 kB)\n",
      "\u001b[K     |████████████████████████████████| 51 kB 290 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting zipline===1.2.0\n",
      "  Downloading zipline-1.2.0.tar.gz (659 kB)\n",
      "\u001b[K     |████████████████████████████████| 659 kB 2.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=1.4.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: seaborn>=0.6.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.10.1)\n",
      "Requirement already satisfied: statsmodels>=0.6.1 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (0.12.0)\n",
      "Requirement already satisfied: IPython>=3.2.3 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from alphalens==0.3.2->-r requirements.txt (line 1)) (7.15.0)\n",
      "Requirement already satisfied: osqp in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 3)) (0.6.1)\n",
      "Requirement already satisfied: ecos>=2 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 3)) (2.0.7.post1)\n",
      "Requirement already satisfied: scs>=1.1.3 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 3)) (2.1.2)\n",
      "Requirement already satisfied: multiprocess in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from cvxpy==1.0.3->-r requirements.txt (line 3)) (0.70.10)\n",
      "Collecting fastcache\n",
      "  Downloading fastcache-1.1.0.tar.gz (20 kB)\n",
      "Collecting toolz\n",
      "  Downloading toolz-0.10.0.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 1.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: decorator>=4.0.6 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 7)) (4.4.2)\n",
      "Requirement already satisfied: nbformat>=4.2 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from plotly==2.2.3->-r requirements.txt (line 7)) (5.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 11)) (2020.4.5.2)\n",
      "Collecting urllib3<1.23,>=1.21.1\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl (132 kB)\n",
      "\u001b[K     |████████████████████████████████| 132 kB 2.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting idna<2.7,>=2.5\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from requests==2.18.4->-r requirements.txt (line 11)) (3.0.4)\n",
      "Collecting numexpr>=2.5.2\n",
      "  Downloading numexpr-2.7.1-cp36-cp36m-macosx_10_6_intel.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pip>=7.1.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (20.0.2)\n",
      "Requirement already satisfied: setuptools>18.0 in /Users/yangjuehan/.local/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (47.1.0)\n",
      "Collecting Logbook>=0.12.5\n",
      "  Downloading Logbook-1.5.3.tar.gz (85 kB)\n",
      "\u001b[K     |████████████████████████████████| 85 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-file>=1.4.1\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Collecting pandas-datareader<0.6,>=0.2.1\n",
      "  Downloading pandas_datareader-0.5.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 3.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: patsy>=0.4.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (0.5.1)\n",
      "Requirement already satisfied: Cython>=0.25.2 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (0.29.21)\n",
      "Collecting cyordereddict>=0.2.2\n",
      "  Downloading cyordereddict-1.0.0.tar.gz (138 kB)\n",
      "\u001b[K     |████████████████████████████████| 138 kB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bottleneck>=1.0.0\n",
      "  Downloading Bottleneck-1.3.2.tar.gz (88 kB)\n",
      "\u001b[K     |████████████████████████████████| 88 kB 2.0 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting contextlib2>=0.4.0\n",
      "  Downloading contextlib2-0.6.0.post1-py2.py3-none-any.whl (9.8 kB)\n",
      "Collecting networkx<2.0,>=1.9.1\n",
      "  Downloading networkx-1.11-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 2.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bcolz<1,>=0.12.1\n",
      "  Downloading bcolz-0.12.1.tar.gz (622 kB)\n",
      "\u001b[K     |████████████████████████████████| 622 kB 3.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click>=4.0.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (7.1.2)\n",
      "Collecting multipledispatch>=0.4.8\n",
      "  Downloading multipledispatch-0.6.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from zipline===1.2.0->-r requirements.txt (line 17)) (1.1.1)\n",
      "Collecting Mako>=1.0.1\n",
      "  Downloading Mako-1.1.3-py2.py3-none-any.whl (75 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 75 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sqlalchemy>=1.0.8\n",
      "  Downloading SQLAlchemy-1.3.19-cp36-cp36m-macosx_10_14_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting alembic>=0.7.7\n",
      "  Downloading alembic-1.4.3-py2.py3-none-any.whl (159 kB)\n",
      "\u001b[K     |████████████████████████████████| 159 kB 2.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting sortedcontainers>=1.4.4\n",
      "  Downloading sortedcontainers-2.2.2-py2.py3-none-any.whl (29 kB)\n",
      "Collecting intervaltree>=2.1.0\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "Collecting lru-dict>=1.1.4\n",
      "  Downloading lru-dict-1.1.6.tar.gz (9.4 kB)\n",
      "Collecting empyrical>=0.4.2\n",
      "  Downloading empyrical-0.5.3.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 3.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from matplotlib>=1.4.0->alphalens==0.3.2->-r requirements.txt (line 1)) (1.2.0)\n",
      "Requirement already satisfied: jedi>=0.10 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.17.0)\n",
      "Requirement already satisfied: appnope; sys_platform == \"darwin\" in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (4.8.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (3.0.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (4.3.3)\n",
      "Requirement already satisfied: backcall in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.0)\n",
      "Requirement already satisfied: pygments in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (2.6.1)\n",
      "Requirement already satisfied: pickleshare in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.7.5)\n",
      "Requirement already satisfied: future in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from osqp->cvxpy==1.0.3->-r requirements.txt (line 3)) (0.18.2)\n",
      "Requirement already satisfied: dill>=0.3.2 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from multiprocess->cvxpy==1.0.3->-r requirements.txt (line 3)) (0.3.2)\n",
      "Requirement already satisfied: ipython-genutils in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (4.6.3)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (3.2.0)\n",
      "Collecting requests-ftp\n",
      "  Downloading requests-ftp-0.3.1.tar.gz (7.8 kB)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Requirement already satisfied: parso>=0.7.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from jedi>=0.10->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.7.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from pexpect; sys_platform != \"win32\"->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.6.0)\n",
      "Requirement already satisfied: wcwidth in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython>=3.2.3->alphalens==0.3.2->-r requirements.txt (line 1)) (0.1.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (0.16.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (19.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/yangjuehan/anaconda3/envs/ml/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema!=2.5.0,>=2.4->nbformat>=4.2->plotly==2.2.3->-r requirements.txt (line 7)) (3.1.0)\n",
      "Building wheels for collected packages: alphalens, cvxpy, pandas, plotly, zipline, fastcache, toolz, Logbook, cyordereddict, bottleneck, bcolz, intervaltree, lru-dict, empyrical, requests-ftp\n",
      "  Building wheel for alphalens (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for alphalens: filename=alphalens-0.3.2-py3-none-any.whl size=18881512 sha256=336053fe017645632533237dc0a2a0410e1cb12eb9ccb3ec3e8031f0daa616bf\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/1c/9e/b8/a502db514b9cf748687129d96fb1f8156f47ef6ab450abf4d0\n",
      "  Building wheel for cvxpy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cvxpy: filename=cvxpy-1.0.3-cp36-cp36m-macosx_10_9_x86_64.whl size=649180 sha256=bf43368eed29d4ac3444eb18c23b905d5a5db625d2320402b6b182492e8ccfe9\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/f6/f9/8a/e5153a07fee191fb3e7ed1ef181c3275d3ee3c094df585741d\n",
      "  Building wheel for pandas (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pandas: filename=pandas-0.18.1-cp36-cp36m-macosx_10_9_x86_64.whl size=6956896 sha256=2cd3e5019734014db155c9c678a944ca3e24b7bd0b4621ad5b50c82625bc261a\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/32/15/fe/873be370134a62a116784ac2e08df00bad7ef4bec708e1f73c\n",
      "  Building wheel for plotly (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for plotly: filename=plotly-2.2.3-py3-none-any.whl size=1122401 sha256=5447d67c882fd5955073c71c89b67dc7451734b3277d5e453624c780f6b1618d\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/3b/63/08/bfacfd43d79c1851c5d03a6e8c27582203c5746fe00579b5db\n",
      "  Building wheel for zipline (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for zipline: filename=zipline-1.2.0-cp36-cp36m-macosx_10_9_x86_64.whl size=1877625 sha256=a55f3f95ccae8e5b45a130135c609f95750304cf8d84d9eb98540a704c75cd95\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/05/13/88/164e4ac4e2662016300b12a0ab0920780bbb725120ccdb858a\n",
      "  Building wheel for fastcache (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fastcache: filename=fastcache-1.1.0-cp36-cp36m-macosx_10_9_x86_64.whl size=18551 sha256=f1ac4574cd65bc850221c13ca8f8e4c7ca8304fd3d4f17084856b5e041383cc7\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/e5/04/07/8deac05af87ab0c90c44179cdcee614092573ec1a82256d656\n",
      "  Building wheel for toolz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for toolz: filename=toolz-0.10.0-py3-none-any.whl size=55575 sha256=67f26da0ad07cfd0ed795de1d1ecaa1f5804924be5fcf1488d1b990fd780b93e\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/fc/da/2e/27e381e9cfc922d078a0a750c7ec72e76df66100e81722516d\n",
      "  Building wheel for Logbook (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for Logbook: filename=Logbook-1.5.3-cp36-cp36m-macosx_10_9_x86_64.whl size=66387 sha256=48c4d28ed6aceb423395f873a424c48bbf875a9f40c7bbbd39dd1d95bcf12d89\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/27/4c/d7/c08e0670a3318441d3bd095149eb6e86e21656f102530ac8b6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Building wheel for cyordereddict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for cyordereddict: filename=cyordereddict-1.0.0-cp36-cp36m-macosx_10_9_x86_64.whl size=54075 sha256=33b68812eb3e5b6c18a72c3c3e8ad33fc7fbc654fb5a15d365dd649f68323d06\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/8d/ff/1a/5f19b34a20e254f738ef53a8469e9e92ee13e66d54de3ea89c\n",
      "  Building wheel for bottleneck (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bottleneck: filename=Bottleneck-1.3.2-cp36-cp36m-macosx_10_9_x86_64.whl size=110201 sha256=640008a52b0124ce8ae5e8532dc0470231021c16252defc914dc0d92f545f6ff\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/f7/a7/14/9be836efed01ac0eb3c125ac006c143b55ebf689269877d0e8\n",
      "  Building wheel for bcolz (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for bcolz: filename=bcolz-0.12.1-cp36-cp36m-macosx_10_9_x86_64.whl size=343076 sha256=c807aa84e0cdc28590a7a253d8be6bd868193f5a0bd59bdfbe60ec01abf7db97\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/9f/51/d6/173c1dabc3904530cd9527026946789e2a065b004916e5c5bb\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26102 sha256=519ec1db21147da4f88fd97d54df68de7b51f601bd709d8f134eed0387d96b1d\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/fc/e6/3f/1616b381f981006664dd5123f06b231bbbb2e7d604a417e2fd\n",
      "  Building wheel for lru-dict (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lru-dict: filename=lru_dict-1.1.6-cp36-cp36m-macosx_10_9_x86_64.whl size=8725 sha256=207e3741914fbaca40f38d5617aea36ea1ebbf888093f82db17dd0923cf54191\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/67/83/2a/833c712d4e578ec0bc7f2ed6eeae77734eb11d2e26cdd583c0\n",
      "  Building wheel for empyrical (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for empyrical: filename=empyrical-0.5.3-py3-none-any.whl size=37090 sha256=a0469fd73b4725e809670efb49e0d062261c9a58ba0856226d76f161512777d1\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/9e/5a/69/db414a9fb2abedee8b26f6efd5d0715e68ab0101caaa5f1a9c\n",
      "  Building wheel for requests-ftp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for requests-ftp: filename=requests_ftp-0.3.1-py3-none-any.whl size=8173 sha256=26ff9ec4f3e46ea1f2cb186d7af77f0696b7d1fc96d390269f78759a3fdc2bc0\n",
      "  Stored in directory: /Users/yangjuehan/Library/Caches/pip/wheels/ea/e4/5d/72f7aad2c589cd6b0fa0c38feda872115bff8512a46d6c7b74\n",
      "Successfully built alphalens cvxpy pandas plotly zipline fastcache toolz Logbook cyordereddict bottleneck bcolz intervaltree lru-dict empyrical requests-ftp\n",
      "\u001b[31mERROR: twine 3.1.1 has requirement requests>=2.20, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: yfinance 0.1.54 has requirement pandas>=0.24, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: yfinance 0.1.54 has requirement requests>=2.20, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: xarray 0.15.1 has requirement pandas>=0.25, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: torchvision 0.6.0 has requirement torch==1.5.0, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorflow 2.3.0 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: tensorboard 2.3.0 has requirement requests<3,>=2.21.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: statsmodels 0.12.0 has requirement pandas>=0.21, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: statsmodels 0.12.0 has requirement scipy>=1.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: seaborn 0.10.1 has requirement pandas>=0.22.0, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: seaborn 0.10.1 has requirement scipy>=1.0.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pyportfolioopt 1.2.4 has requirement pandas>=0.19, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: pyportfolioopt 1.2.4 has requirement scipy<2.0,>=1.3, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: mxnet 1.6.0 has requirement requests<3,>=2.20.0, but you'll have requests 2.18.4 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: dm-tree 0.1.5 has requirement six>=1.12.0, but you'll have six 1.11.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: arviz 0.8.3 has requirement pandas>=0.23, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: arch 4.15 has requirement pandas>=0.23, but you'll have pandas 0.18.1 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: arch 4.15 has requirement scipy>=1.0.1, but you'll have scipy 1.0.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: numpy, six, python-dateutil, pytz, pandas, scipy, alphalens, colour, fastcache, toolz, cvxpy, urllib3, idna, requests, plotly, pyparsing, scikit-learn, numexpr, tables, tqdm, Logbook, requests-file, requests-ftp, pandas-datareader, cyordereddict, bottleneck, contextlib2, networkx, bcolz, multipledispatch, Mako, sqlalchemy, python-editor, alembic, sortedcontainers, intervaltree, lru-dict, empyrical, zipline\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.18.4\n",
      "    Uninstalling numpy-1.18.4:\n",
      "      Successfully uninstalled numpy-1.18.4\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.15.0\n",
      "    Uninstalling six-1.15.0:\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-89d1e971752d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcvx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mproject_tests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/cvxpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1.0.31\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstraints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNonPos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZero\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSOC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPSD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/cvxpy/atoms/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcummax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcummax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdist_ratio\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdist_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meye_minus_inv\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meye_minus_inv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolvent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/cvxpy/atoms/cummax.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \"\"\"\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAtom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matoms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis_atom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAxisAtom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/cvxpy/atoms/atom.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutilities\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterface\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mintf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcvxpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpressions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcvxtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/cvxpy/utilities/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcanonical\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanonical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msign\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/cvxpy/utilities/grad.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Utility functions for computing gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/scipy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_config\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mshow_numpy_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mshow_numpy_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     raise ImportError(\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/multiarray.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_multiarray_umath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.6/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtextwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m from numpy.core._multiarray_umath import (\n\u001b[0m\u001b[1;32m      8\u001b[0m     add_docstring, implement_array_function, _get_implementing_args)\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inspect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetargspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cvxpy as cvx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import project_tests\n",
    "import project_helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Bundle\n",
    "We'll be using Zipline to handle our data. We've created a end of day data bundle for this project. Run the cell below to register this data bundle in zipline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import project_helper\n",
    "from zipline.data import bundles\n",
    "\n",
    "os.environ['ZIPLINE_ROOT'] = os.path.join(os.getcwd(), '..', '..', 'data', 'project_4_eod')\n",
    "\n",
    "ingest_func = bundles.csvdir.csvdir_equities(['daily'], project_helper.EOD_BUNDLE_NAME)\n",
    "bundles.register(project_helper.EOD_BUNDLE_NAME, ingest_func)\n",
    "\n",
    "print('Data Registered')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline Engine\n",
    "We'll be using Zipline's pipeline package to access our data for this project. To use it, we must build a pipeline engine. Run the cell below to build the engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline import Pipeline\n",
    "from zipline.pipeline.factors import AverageDollarVolume\n",
    "from zipline.utils.calendars import get_calendar\n",
    "\n",
    "\n",
    "universe = AverageDollarVolume(window_length=120).top(500) \n",
    "trading_calendar = get_calendar('NYSE') \n",
    "bundle_data = bundles.load(project_helper.EOD_BUNDLE_NAME)\n",
    "engine = project_helper.build_pipeline_engine(bundle_data, trading_calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "With the pipeline engine built, let's get the stocks at the end of the period in the universe we're using. We'll use these tickers to generate the returns data for the our risk model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe_end_date = pd.Timestamp('2016-01-05', tz='UTC')\n",
    "\n",
    "universe_tickers = engine\\\n",
    "    .run_pipeline(\n",
    "        Pipeline(screen=universe),\n",
    "        universe_end_date,\n",
    "        universe_end_date)\\\n",
    "    .index.get_level_values(1)\\\n",
    "    .values.tolist()\n",
    "    \n",
    "universe_tickers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Returns\n",
    "Not that we have our pipeline built, let's access the returns data. We'll start by building a data portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.data.data_portal import DataPortal\n",
    "\n",
    "\n",
    "data_portal = DataPortal(\n",
    "    bundle_data.asset_finder,\n",
    "    trading_calendar=trading_calendar,\n",
    "    first_trading_day=bundle_data.equity_daily_bar_reader.first_trading_day,\n",
    "    equity_minute_reader=None,\n",
    "    equity_daily_reader=bundle_data.equity_daily_bar_reader,\n",
    "    adjustment_reader=bundle_data.adjustment_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the code easier to read, we've built the helper function `get_pricing` to get the pricing from the data portal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pricing(data_portal, trading_calendar, assets, start_date, end_date, field='close'):\n",
    "    end_dt = pd.Timestamp(end_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "    start_dt = pd.Timestamp(start_date.strftime('%Y-%m-%d'), tz='UTC', offset='C')\n",
    "\n",
    "    end_loc = trading_calendar.closes.index.get_loc(end_dt)\n",
    "    start_loc = trading_calendar.closes.index.get_loc(start_dt)\n",
    "\n",
    "    return data_portal.get_history_window(\n",
    "        assets=assets,\n",
    "        end_dt=end_dt,\n",
    "        bar_count=end_loc - start_loc,\n",
    "        frequency='1d',\n",
    "        field=field,\n",
    "        data_frequency='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's get returns data for our risk model using the `get_pricing` function. For this model, we'll be looking back to 5 years of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "five_year_returns = \\\n",
    "    get_pricing(\n",
    "        data_portal,\n",
    "        trading_calendar,\n",
    "        universe_tickers,\n",
    "        universe_end_date - pd.DateOffset(years=5),\n",
    "        universe_end_date)\\\n",
    "    .pct_change()[1:].fillna(0)\n",
    "\n",
    "five_year_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Risk Model\n",
    "It's time to build the risk model. You'll be creating a statistical risk model using PCA. So, the first thing is building the PCA model.\n",
    "## Fit PCA\n",
    "Implement `fit_pca` to fit a PCA model to the returns data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "def fit_pca(returns, num_factor_exposures, svd_solver):\n",
    "    \"\"\"\n",
    "    Fit PCA model with returns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    num_factor_exposures : int\n",
    "        Number of factors for PCA\n",
    "    svd_solver: str\n",
    "        The solver to use for the PCA model\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pca : PCA\n",
    "        Model fit to returns\n",
    "    \"\"\"\n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_fit_pca(fit_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what the model looks like. First, we'll look at the PCA components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_factor_exposures = 20\n",
    "pca = fit_pca(five_year_returns, num_factor_exposures, 'full')\n",
    "\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also look at the PCA's percent of variance explained by each factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(np.arange(num_factor_exposures), pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the first factor dominates. The precise definition of each factor in a latent model is unknown, however we can guess at the likely interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Betas\n",
    "Implement `factor_betas` to get the factor betas from the PCA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_betas(pca, factor_beta_indices, factor_beta_columns):\n",
    "    \"\"\"\n",
    "    Get the factor betas from the PCA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca : PCA\n",
    "        Model fit to returns\n",
    "    factor_beta_indices : 1 dimensional Ndarray\n",
    "        Factor beta indices\n",
    "    factor_beta_columns : 1 dimensional Ndarray\n",
    "        Factor beta columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor_betas : DataFrame\n",
    "        Factor betas\n",
    "    \"\"\"\n",
    "    assert len(factor_beta_indices.shape) == 1\n",
    "    assert len(factor_beta_columns.shape) == 1\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_factor_betas(factor_betas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's view the factor betas from this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model = {}\n",
    "risk_model['factor_betas'] = factor_betas(pca, five_year_returns.columns.values, np.arange(num_factor_exposures))\n",
    "\n",
    "risk_model['factor_betas']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Returns\n",
    "Implement `factor_returns` to get the factor returns from the PCA model using the returns data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_returns(pca, returns, factor_return_indices, factor_return_columns):\n",
    "    \"\"\"\n",
    "    Get the factor returns from the PCA model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pca : PCA\n",
    "        Model fit to returns\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    factor_return_indices : 1 dimensional Ndarray\n",
    "        Factor return indices\n",
    "    factor_return_columns : 1 dimensional Ndarray\n",
    "        Factor return columns\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns\n",
    "    \"\"\"\n",
    "    assert len(factor_return_indices.shape) == 1\n",
    "    assert len(factor_return_columns.shape) == 1\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_factor_returns(factor_returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what these factor returns looks like over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model['factor_returns'] = factor_returns(\n",
    "    pca,\n",
    "    five_year_returns,\n",
    "    five_year_returns.index,\n",
    "    np.arange(num_factor_exposures))\n",
    "\n",
    "risk_model['factor_returns'].cumsum().plot(legend=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factor Covariance Matrix\n",
    "Implement `factor_cov_matrix` to get the factor covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def factor_cov_matrix(factor_returns, ann_factor):\n",
    "    \"\"\"\n",
    "    Get the factor covariance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns\n",
    "    ann_factor : int\n",
    "        Annualization factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor_cov_matrix : 2 dimensional Ndarray\n",
    "        Factor covariance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_factor_cov_matrix(factor_cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_factor = 252\n",
    "risk_model['factor_cov_matrix'] = factor_cov_matrix(risk_model['factor_returns'], ann_factor)\n",
    "\n",
    "risk_model['factor_cov_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idiosyncratic Variance Matrix\n",
    "Implement `idiosyncratic_var_matrix` to get the idiosyncratic variance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiosyncratic_var_matrix(returns, factor_returns, factor_betas, ann_factor):\n",
    "    \"\"\"\n",
    "    Get the idiosyncratic variance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns\n",
    "    factor_betas : DataFrame\n",
    "        Factor betas\n",
    "    ann_factor : int\n",
    "        Annualization factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idiosyncratic_var_matrix : DataFrame\n",
    "        Idiosyncratic variance matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_idiosyncratic_var_matrix(idiosyncratic_var_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model['idiosyncratic_var_matrix'] = idiosyncratic_var_matrix(five_year_returns, risk_model['factor_returns'], risk_model['factor_betas'], ann_factor)\n",
    "\n",
    "risk_model['idiosyncratic_var_matrix']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idiosyncratic Variance Vector\n",
    "Implement `idiosyncratic_var_vector` to get the idiosyncratic variance Vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idiosyncratic_var_vector(returns, idiosyncratic_var_matrix):\n",
    "    \"\"\"\n",
    "    Get the idiosyncratic variance vector\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    returns : DataFrame\n",
    "        Returns for each ticker and date\n",
    "    idiosyncratic_var_matrix : DataFrame\n",
    "        Idiosyncratic variance matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idiosyncratic_var_vector : DataFrame\n",
    "        Idiosyncratic variance Vector\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_idiosyncratic_var_vector(idiosyncratic_var_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_model['idiosyncratic_var_vector'] = idiosyncratic_var_vector(five_year_returns, risk_model['idiosyncratic_var_matrix'])\n",
    "\n",
    "risk_model['idiosyncratic_var_vector']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict using the Risk Model\n",
    "Using the data we calculated in the risk model, implement `predict_portfolio_risk` to predict the portfolio risk using the formula $ \\sqrt{X^{T}(BFB^{T} + S)X} $ where:\n",
    "- $ X $ is the portfolio weights\n",
    "- $ B $ is the factor betas\n",
    "- $ F $ is the factor covariance matrix\n",
    "- $ S $ is the idiosyncratic variance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_portfolio_risk(factor_betas, factor_cov_matrix, idiosyncratic_var_matrix, weights):\n",
    "    \"\"\"\n",
    "    Get the predicted portfolio risk\n",
    "    \n",
    "    Formula for predicted portfolio risk is sqrt(X.T(BFB.T + S)X) where:\n",
    "      X is the portfolio weights\n",
    "      B is the factor betas\n",
    "      F is the factor covariance matrix\n",
    "      S is the idiosyncratic variance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_betas : DataFrame\n",
    "        Factor betas\n",
    "    factor_cov_matrix : 2 dimensional Ndarray\n",
    "        Factor covariance matrix\n",
    "    idiosyncratic_var_matrix : DataFrame\n",
    "        Idiosyncratic variance matrix\n",
    "    weights : DataFrame\n",
    "        Portfolio weights\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predicted_portfolio_risk : float\n",
    "        Predicted portfolio risk\n",
    "    \"\"\"\n",
    "    assert len(factor_cov_matrix.shape) == 2\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_predict_portfolio_risk(predict_portfolio_risk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what the portfolio risk would be if we had even weights across all stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = pd.DataFrame(np.repeat(1/len(universe_tickers), len(universe_tickers)), universe_tickers)\n",
    "\n",
    "predict_portfolio_risk(\n",
    "    risk_model['factor_betas'],\n",
    "    risk_model['factor_cov_matrix'],\n",
    "    risk_model['idiosyncratic_var_matrix'],\n",
    "    all_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Alpha Factors\n",
    "With the profile risk calculated, it's time to start working on the alpha factors. In this project, we'll create the following factors:\n",
    "- Momentum 1 Year Factor\n",
    "- Mean Reversion 5 Day Sector Neutral Factor\n",
    "- Mean Reversion 5 Day Sector Neutral Smoothed Factor\n",
    "- Overnight Sentiment Factor\n",
    "- Overnight Sentiment Smoothed Factor\n",
    "\n",
    "## Momentum 1 Year Factor\n",
    "Each factor will have a hypothesis that goes with it. For this factor, it is \"Higher past 12-month (252 days) returns are proportional to future return.\" Using that hypothesis, we've generated this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import Returns\n",
    "\n",
    "def momentum_1yr(window_length, universe, sector):\n",
    "    return Returns(window_length=window_length, mask=universe) \\\n",
    "        .demean(groupby=sector) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reversion 5 Day Sector Neutral Factor\n",
    "Now it's time for you to implement `mean_reversion_5day_sector_neutral` using the hypothesis \"Short-term outperformers(underperformers) compared to their sector will revert.\" Use the returns data from `universe`, demean using the sector data to partition, rank, then converted to a zscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_reversion_5day_sector_neutral(window_length, universe, sector):\n",
    "    \"\"\"\n",
    "    Generate the mean reversion 5 day sector neutral factor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window_length : int\n",
    "        Returns window length\n",
    "    universe : Zipline Filter\n",
    "        Universe of stocks filter\n",
    "    sector : Zipline Classifier\n",
    "        Sector classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor : Zipline Factor\n",
    "        Mean reversion 5 day sector neutral factor\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_mean_reversion_5day_sector_neutral(mean_reversion_5day_sector_neutral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what some of the factor data looks like. For calculating factors, we'll be looking back 2 years.\n",
    "\n",
    "**Note:** _Going back 2 years falls on a day when the market is closed. Pipeline package doesn't handle start or end dates that don't fall on days when the market is open. To fix this, we went back 2 extra days to fall on the next day when the market is open._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_start_date = universe_end_date - pd.DateOffset(years=2, days=2)\n",
    "sector = project_helper.Sector()\n",
    "window_length = 5\n",
    "\n",
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral(window_length, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral')\n",
    "engine.run_pipeline(pipeline, factor_start_date, universe_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Reversion 5 Day Sector Neutral Smoothed Factor\n",
    "Taking the output of the previous factor, let's create a smoothed version. Implement `mean_reversion_5day_sector_neutral_smoothed` to generate a mean reversion 5 fay sector neutral smoothed factor. Call the `mean_reversion_5day_sector_neutral` function to get the unsmoothed factor, then use `SimpleMovingAverage` function to smooth it. You'll have to apply rank and zscore again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.factors import SimpleMovingAverage\n",
    "\n",
    "def mean_reversion_5day_sector_neutral_smoothed(window_length, universe, sector):\n",
    "    \"\"\"\n",
    "    Generate the mean reversion 5 day sector neutral smoothed factor\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    window_length : int\n",
    "        Returns window length\n",
    "    universe : Zipline Filter\n",
    "        Universe of stocks filter\n",
    "    sector : Zipline Classifier\n",
    "        Sector classifier\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    factor : Zipline Factor\n",
    "        Mean reversion 5 day sector neutral smoothed factor\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_mean_reversion_5day_sector_neutral_smoothed(mean_reversion_5day_sector_neutral_smoothed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what some of the smoothed data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral_smoothed(5, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\n",
    "engine.run_pipeline(pipeline, factor_start_date, universe_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Sentiment Factor\n",
    "For this factor, were using the hypothesis from the paper [_Overnight Returns and Firm-Specific Investor Sentiment_](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zipline.pipeline.data import USEquityPricing\n",
    "\n",
    "\n",
    "class CTO(Returns):\n",
    "    \"\"\"\n",
    "    Computes the overnight return, per hypothesis from\n",
    "    https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2554010\n",
    "    \"\"\"\n",
    "    inputs = [USEquityPricing.open, USEquityPricing.close]\n",
    "    \n",
    "    def compute(self, today, assets, out, opens, closes):\n",
    "        \"\"\"\n",
    "        The opens and closes matrix is 2 rows x N assets, with the most recent at the bottom.\n",
    "        As such, opens[-1] is the most recent open, and closes[0] is the earlier close\n",
    "        \"\"\"\n",
    "        out[:] = (opens[-1] - closes[0]) / closes[0]\n",
    "\n",
    "        \n",
    "class TrailingOvernightReturns(Returns):\n",
    "    \"\"\"\n",
    "    Sum of trailing 1m O/N returns\n",
    "    \"\"\"\n",
    "    window_safe = True\n",
    "    \n",
    "    def compute(self, today, asset_ids, out, cto):\n",
    "        out[:] = np.nansum(cto, axis=0)\n",
    "\n",
    "        \n",
    "def overnight_sentiment(cto_window_length, trail_overnight_returns_window_length, universe):\n",
    "    cto_out = CTO(mask=universe, window_length=cto_window_length)\n",
    "    return TrailingOvernightReturns(inputs=[cto_out], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overnight Sentiment Smoothed Factor\n",
    "Just like the factor you implemented, we'll also smooth this factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overnight_sentiment_smoothed(cto_window_length, trail_overnight_returns_window_length, universe):\n",
    "    unsmoothed_factor = overnight_sentiment(cto_window_length, trail_overnight_returns_window_length, universe)\n",
    "    return SimpleMovingAverage(inputs=[unsmoothed_factor], window_length=trail_overnight_returns_window_length) \\\n",
    "        .rank() \\\n",
    "        .zscore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the Factors to a single Pipeline\n",
    "With all the factor implementations done, let's add them to a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "universe = AverageDollarVolume(window_length=120).top(500)\n",
    "sector = project_helper.Sector()\n",
    "\n",
    "pipeline = Pipeline(screen=universe)\n",
    "pipeline.add(\n",
    "    momentum_1yr(252, universe, sector),\n",
    "    'Momentum_1YR')\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral(5, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral')\n",
    "pipeline.add(\n",
    "    mean_reversion_5day_sector_neutral_smoothed(5, universe, sector),\n",
    "    'Mean_Reversion_5Day_Sector_Neutral_Smoothed')\n",
    "pipeline.add(\n",
    "    overnight_sentiment(2, 5, universe),\n",
    "    'Overnight_Sentiment')\n",
    "pipeline.add(\n",
    "    overnight_sentiment_smoothed(2, 5, universe),\n",
    "    'Overnight_Sentiment_Smoothed')\n",
    "all_factors = engine.run_pipeline(pipeline, factor_start_date, universe_end_date)\n",
    "\n",
    "all_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Alpha Factors\n",
    "*Note:* _We're evaluating the alpha factors using delay of 1_\n",
    "## Get Pricing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alphalens as al\n",
    "\n",
    "assets = all_factors.index.levels[1].values.tolist()\n",
    "pricing = get_pricing(\n",
    "    data_portal,\n",
    "    trading_calendar,\n",
    "    assets,\n",
    "    factor_start_date,\n",
    "    universe_end_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format alpha factors and pricing for Alphalens\n",
    "In order to use a lot of the alphalens functions, we need to aligned the indices and convert the time to unix timestamp. In this next cell, we'll do just that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_factor_data = {\n",
    "    factor: al.utils.get_clean_factor_and_forward_returns(factor=factor_data, prices=pricing, periods=[1])\n",
    "    for factor, factor_data in all_factors.iteritems()}\n",
    "\n",
    "unixt_factor_data = {\n",
    "    factor: factor_data.set_index(pd.MultiIndex.from_tuples(\n",
    "        [(x.timestamp(), y) for x, y in factor_data.index.values],\n",
    "        names=['date', 'asset']))\n",
    "    for factor, factor_data in clean_factor_data.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantile Analysis\n",
    "### Factor Returns\n",
    "Let's view the factor returns over time. We should be seeing it generally move up and to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in clean_factor_data.items():\n",
    "    ls_factor_returns[factor] = al.performance.factor_returns(factor_data).iloc[:, 0]\n",
    "\n",
    "(1+ls_factor_returns).cumprod().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basis Points Per Day per Quantile\n",
    "It is not enough to look just at the factor weighted return. A good alpha is also monotonic in quantiles. Let's looks the basis points for the factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qr_factor_returns = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in unixt_factor_data.items():\n",
    "    qr_factor_returns[factor] = al.performance.mean_return_by_quantile(factor_data)[0].iloc[:, 0]\n",
    "\n",
    "(10000*qr_factor_returns).plot.bar(\n",
    "    subplots=True,\n",
    "    sharey=True,\n",
    "    layout=(4,2),\n",
    "    figsize=(14, 14),\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe?\n",
    "\n",
    "- None of these alphas are **strictly monotonic**; this should lead you to question why this is? Further research and refinement of the alphas needs to be done. What is it about these alphas that leads to the highest ranking stocks in all alphas except MR 5D smoothed to *not* perform the best.\n",
    "- The majority of the return is coming from the **short side** in all these alphas. The negative return in quintile 1 is very large in all alphas. This could also a cause for concern becuase when you short stocks, you need to locate the short; shorts can be expensive or not available at all.\n",
    "- If you look at the magnitude of the return spread (i.e., Q1 minus Q5), we are working with daily returns in the 0.03%, i.e., **3 basis points**, neighborhood *before all transaction costs, shorting costs, etc.*. Assuming 252 days in a year, that's 7.56% return annualized. Transaction costs may cut this in half. As such, it should be clear that these alphas can only survive in an institutional setting and that leverage will likely need to be applied in order to achieve an attractive return.\n",
    "\n",
    "## Turnover Analysis\n",
    "\n",
    "Without doing a full and formal backtest, we can analyze how stable the alphas are over time. Stability in this sense means that from period to period, the alpha ranks do not change much. Since trading is costly, we always prefer, all other things being equal, that the ranks do not change significantly per period. We can measure this with the **factor rank autocorrelation (FRA)**.\n",
    "\n",
    "[alphalens.performance.factor_rank_autocorrelation](https://quantopian.github.io/alphalens/alphalens.html?highlight=factor_rank_autocorrelation#alphalens.performance.factor_rank_autocorrelation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_FRA = pd.DataFrame()\n",
    "\n",
    "for factor, factor_data in unixt_factor_data.items():\n",
    "    ls_FRA[factor] = al.performance.factor_rank_autocorrelation(factor_data)\n",
    "\n",
    "ls_FRA.plot(title=\"Factor Rank Autocorrelation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sharpe Ratio of the Alphas\n",
    "\n",
    "The last analysis we'll do on the factors will be sharpe ratio. Implement `sharpe_ratio` to calculate the sharpe ratio of factor returns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sharpe_ratio(factor_returns, annualization_factor):\n",
    "    \"\"\"\n",
    "    Get the sharpe ratio for each factor for the entire period\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    factor_returns : DataFrame\n",
    "        Factor returns for each factor and date\n",
    "    annualization_factor: float\n",
    "        Annualization Factor\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    sharpe_ratio : Pandas Series of floats\n",
    "        Sharpe ratio\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO: Implement function\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "project_tests.test_sharpe_ratio(sharpe_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "Let's see what the sharpe ratio for the factors are. Generally, a Sharpe Ratio of near 1.0 or higher is an acceptable single alpha for this universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_annualization_factor = np.sqrt(252)\n",
    "sharpe_ratio(ls_factor_returns, daily_annualization_factor).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question: What do you think would happen if we smooth the momentum factor? Would the performance increase, decrease, or no major change? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_#TODO: Put Answer In this Cell_\n",
    "\n",
    "**Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Combined Alpha Vector\n",
    "\n",
    "To use these alphas in a portfolio, we need to combine them somehow so we get a single score per stock. This is a area where machine learning can be very helpful. In this module, however, we will take the simplest approach of combination: simply averaging the scores from each alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_factors = all_factors.columns[[1, 2, 4]]\n",
    "print('Selected Factors: {}'.format(', '.join(selected_factors)))\n",
    "\n",
    "all_factors['alpha_vector'] = all_factors[selected_factors].mean(axis=1)\n",
    "alphas = all_factors[['alpha_vector']]\n",
    "alpha_vector = alphas.loc[all_factors.index.get_level_values(0)[-1]]\n",
    "alpha_vector.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimal Portfolio Constrained by Risk Model\n",
    "You have an alpha model and a risk model. Let's find a portfolio that trades as close as possible to the alpha model but limiting risk as measured by the risk model. You'll be building thie optimizer for this portfolio. To help you out. we have provided you with an abstract class called `AbstractOptimalHoldings`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "\n",
    "class AbstractOptimalHoldings(ABC):    \n",
    "    @abstractmethod\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _get_constraints(self, weights, factor_betas, risk):\n",
    "        \"\"\"\n",
    "        Get the constraints\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        factor_betas : 2 dimensional Ndarray\n",
    "            Factor betas\n",
    "        risk: CVXPY Atom\n",
    "            Predicted variance of the portfolio returns\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        constraints : List of CVXPY Constraint\n",
    "            Constraints\n",
    "        \"\"\"\n",
    "        \n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    def _get_risk(self, weights, factor_betas, alpha_vector_index, factor_cov_matrix, idiosyncratic_var_vector):\n",
    "        f = factor_betas.loc[alpha_vector_index].values.T * weights\n",
    "        X = factor_cov_matrix\n",
    "        S = np.diag(idiosyncratic_var_vector.loc[alpha_vector_index].values.flatten())\n",
    "\n",
    "        return cvx.quad_form(f, X) + cvx.quad_form(weights, S)\n",
    "    \n",
    "    def find(self, alpha_vector, factor_betas, factor_cov_matrix, idiosyncratic_var_vector):\n",
    "        weights = cvx.Variable(len(alpha_vector))\n",
    "        risk = self._get_risk(weights, factor_betas, alpha_vector.index, factor_cov_matrix, idiosyncratic_var_vector)\n",
    "        \n",
    "        obj = self._get_obj(weights, alpha_vector)\n",
    "        constraints = self._get_constraints(weights, factor_betas.loc[alpha_vector.index].values, risk)\n",
    "        \n",
    "        prob = cvx.Problem(obj, constraints)\n",
    "        prob.solve(max_iters=500)\n",
    "\n",
    "        optimal_weights = np.asarray(weights.value).flatten()\n",
    "        \n",
    "        return pd.DataFrame(data=optimal_weights, index=alpha_vector.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective and Constraints\n",
    "Using this class as a base class, you'll implement the `OptimalHoldings` class. There's two functions that need to be implemented in this class, the `_get_obj` and `_get_constraints` functions.\n",
    "\n",
    "The `_get_obj` function should return an CVXPY objective function that maximizes $ \\alpha^T * x \\\\ $, where $ x $ is the portfolio weights and $ \\alpha $ is the alpha vector.\n",
    "\n",
    "The `_get_constraints` function should return a list of the following constraints:\n",
    "- $ r \\leq risk_{\\text{cap}}^2 \\\\ $\n",
    "- $ B^T * x \\preceq factor_{\\text{max}} \\\\ $\n",
    "- $ B^T * x \\succeq factor_{\\text{min}} \\\\ $\n",
    "- $ x^T\\mathbb{1} = 0 \\\\ $\n",
    "- $ \\|x\\|_1 \\leq 1 \\\\ $\n",
    "- $ x \\succeq weights_{\\text{min}} \\\\ $\n",
    "- $ x \\preceq weights_{\\text{max}} $\n",
    "\n",
    "Where $ x $ is the portfolio weights, $ B $ is the factor betas, and $ r $ is the portfolio risk\n",
    "\n",
    "The first constraint is that the predicted risk be less than some maximum limit. The second and third constraints are on the maximum and minimum portfolio factor exposures. The fourth constraint is the \"market neutral constraint: the sum of the weights must be zero. The fifth constraint is the leverage constraint: the sum of the absolute value of the weights must be less than or equal to 1.0. The last are some minimum and maximum limits on individual holdings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalHoldings(AbstractOptimalHoldings):\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        assert(len(alpha_vector.columns) == 1)\n",
    "\n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _get_constraints(self, weights, factor_betas, risk):\n",
    "        \"\"\"\n",
    "        Get the constraints\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        factor_betas : 2 dimensional Ndarray\n",
    "            Factor betas\n",
    "        risk: CVXPY Atom\n",
    "            Predicted variance of the portfolio returns\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        constraints : List of CVXPY Constraint\n",
    "            Constraints\n",
    "        \"\"\"\n",
    "        assert(len(factor_betas.shape) == 2)\n",
    "        \n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __init__(self, risk_cap=0.05, factor_max=10.0, factor_min=-10.0, weights_max=0.55, weights_min=-0.55):\n",
    "        self.risk_cap=risk_cap\n",
    "        self.factor_max=factor_max\n",
    "        self.factor_min=factor_min\n",
    "        self.weights_max=weights_max\n",
    "        self.weights_min=weights_min\n",
    "\n",
    "\n",
    "project_tests.test_optimal_holdings_get_obj(OptimalHoldings)\n",
    "project_tests.test_optimal_holdings_get_constraints(OptimalHoldings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data\n",
    "With the `OptimalHoldings` class implemented, let's see the weights it generates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights = OptimalHoldings().find(alpha_vector, risk_model['factor_betas'], risk_model['factor_cov_matrix'], risk_model['idiosyncratic_var_vector'])\n",
    "\n",
    "optimal_weights.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yikes. It put most of the weight in a few stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.get_factor_exposures(risk_model['factor_betas'], optimal_weights).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with a Regularization Parameter\n",
    "In order to enforce diversification, we'll use regularization in the objective function. We'll create a new class called `OptimalHoldingsRegualization` which gets its constraints from the `OptimalHoldings` class. In this new class, implement the `_get_obj` function to return a CVXPY objective function that maximize $ \\alpha^T * x + \\lambda\\|x\\|_2\\\\ $, where $ x $ is the portfolio weights, $ \\alpha $ is the alpha vector, and $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "**Note:** * $ \\lambda $ is located in `self.lambda_reg`. *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalHoldingsRegualization(OptimalHoldings):\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        assert(len(alpha_vector.columns) == 1)\n",
    "        \n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def __init__(self, lambda_reg=0.5, risk_cap=0.05, factor_max=10.0, factor_min=-10.0, weights_max=0.55, weights_min=-0.55):\n",
    "        self.lambda_reg = lambda_reg\n",
    "        self.risk_cap=risk_cap\n",
    "        self.factor_max=factor_max\n",
    "        self.factor_min=factor_min\n",
    "        self.weights_max=weights_max\n",
    "        self.weights_min=weights_min\n",
    "        \n",
    "\n",
    "project_tests.test_optimal_holdings_regualization_get_obj(OptimalHoldingsRegualization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_1 = OptimalHoldingsRegualization(lambda_reg=5.0).find(alpha_vector, risk_model['factor_betas'], risk_model['factor_cov_matrix'], risk_model['idiosyncratic_var_vector'])\n",
    "\n",
    "optimal_weights_1.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice. Well diversified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.get_factor_exposures(risk_model['factor_betas'], optimal_weights_1).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with a Strict Factor Constraints and Target Weighting\n",
    "Another common formulation is to take a predefined target weighting, $x^*$ (e.g., a quantile portfolio), and solve to get as close to that portfolio while respecting portfolio-level constraints. For this next class, `OptimalHoldingsStrictFactor`, you'll implement the `_get_obj` function to minimize on on $ \\|x - x^*\\|_2 $, where $ x $ is the portfolio weights  $ x^* $ is the target weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimalHoldingsStrictFactor(OptimalHoldings):\n",
    "    def _get_obj(self, weights, alpha_vector):\n",
    "        \"\"\"\n",
    "        Get the objective function\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        weights : CVXPY Variable\n",
    "            Portfolio weights\n",
    "        alpha_vector : DataFrame\n",
    "            Alpha vector\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        objective : CVXPY Objective\n",
    "            Objective function\n",
    "        \"\"\"\n",
    "        assert(len(alpha_vector.columns) == 1)\n",
    "        \n",
    "        #TODO: Implement function\n",
    "        \n",
    "        return None\n",
    "\n",
    "\n",
    "project_tests.test_optimal_holdings_strict_factor_get_obj(OptimalHoldingsStrictFactor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_2 = OptimalHoldingsStrictFactor(\n",
    "    weights_max=0.02,\n",
    "    weights_min=-0.02,\n",
    "    risk_cap=0.0015,\n",
    "    factor_max=0.015,\n",
    "    factor_min=-0.015).find(alpha_vector, risk_model['factor_betas'], risk_model['factor_cov_matrix'], risk_model['idiosyncratic_var_vector'])\n",
    "\n",
    "optimal_weights_2.plot.bar(legend=None, title='Portfolio % Holdings by Stock')\n",
    "x_axis = plt.axes().get_xaxis()\n",
    "x_axis.set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_helper.get_factor_exposures(risk_model['factor_betas'], optimal_weights_2).plot.bar(\n",
    "    title='Portfolio Net Factor Exposures',\n",
    "    legend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "Now that you're done with the project, it's time to submit it. Click the submit button in the bottom right. One of our reviewers will give you feedback on your project with a pass or not passed grade. You can continue to the next section while you wait for feedback."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
